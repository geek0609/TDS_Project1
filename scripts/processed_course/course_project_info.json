[
  {
    "id": "course_project_1",
    "title": "Project 1 - LLM-based Automation Agent",
    "filename": "project-1.md",
    "relative_path": "project-1.md",
    "category": "project",
    "content": "# Project 1 - LLM-based Automation Agent\n\nThis project is due on 16 Feb 2025 EoD IST. Results will be announced by 26 Feb 2025.\n\nFor questions, [use this Discourse thread](https://discourse.onlinedegree.iitm.ac.in/t/project-1-llm-based-automation-agent-discussion-thread-tds-jan-2025/164277).\n\n## Background\n\nYou have joined the operations team at **DataWorks Solutions**, a company that processes large volumes of log files, reports, and code artifacts to generate actionable insights for internal stakeholders. In order to improve operational efficiency and consistency, the company has mandated that routine tasks be automated and integrated into their Continuous Integration (CI) pipeline.\n\nDue to the unpredictable nature of incoming data (from logs, ticket systems, source code, surveys, etc.) the team has decided to use a Large Language Model (LLM) as an intermediate transformer. In this role, the LLM will perform small, reasonably deterministic tasks.\n\nYour assignment is to build an automation agent that accepts plain‑English tasks, carries out the required (multi‑step) process leveraging an LLM where required. The finished processing artifacts must be exactly verifiable against pre‑computed expected results.\n\n## Create an API\n\nWrite an application that exposes an API with the following endpoints:\n\n- **POST `/run?task=<task description>`**\n Executes a plain‑English task. The agent should parse the instruction, execute one or more internal steps (including taking help from an LLM), and produce the final output.\n - If successful, return a HTTP 200 OK response\n - If unsuccessful because of an error in the task, return a HTTP 400 Bad Request response\n - If unsuccessful because of an error in the agent, return a HTTP 500 Internal Server Error response\n - The body may optionally contain any useful information in each of these cases\n- **GET `/read?path=<file path>`**\n Returns the content of the specified file. This is critical for verification of the exact output.\n - If successful, return a HTTP 200 OK response with the file content as plain text\n - If the file does not exist, return a HTTP 404 Not Found response and an empty body\n\n## Phase A: Handle Operations Tasks\n\nThe DataWorks operations team has identified these tasks that need to be automated:\n\n- **A1**. Install `uv` (if required) and run `https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py` with `${user.email}` as the only argument. (**NOTE**: This will generate data files required for the next tasks.)\n- **A2**. Format the contents of `/data/format.md` using `prettier@3.4.2`, updating the file in-place\n- **A3**. The file `/data/dates.txt` contains a list of dates, one per line. Count the number of Wednesdays in the list, and write just the number to `/data/dates-wednesdays.txt`\n- **A4**. Sort the array of contacts in `/data/contacts.json` by `last_name`, then `first_name`, and write the result to `/data/contacts-sorted.json`\n- **A5**. Write the first line of the 10 most recent `.log` file in `/data/logs/` to `/data/logs-recent.txt`, most recent first\n- **A6**. Find all Markdown (`.md`) files in `/data/docs/`. For each file, extract the first occurrance of each H1 (i.e. a line starting with `# `). Create an index file `/data/docs/index.json` that maps each filename (without the `/data/docs/` prefix) to its title (e.g. `{\"README.md\": \"Home\", \"path/to/large-language-models.md\": \"Large Language Models\", ...}`)\n- **A7**. `/data/email.txt` contains an email message. Pass the content to an LLM with instructions to extract the sender's email address, and write just the email address to `/data/email-sender.txt`\n- **A8**. `/data/credit-card.png` contains a credit card number. Pass the image to an LLM, have it extract the card number, and write it without spaces to `/data/credit-card.txt`\n- **A9**. `/data/comments.txt` contains a list of comments, one per line. Using embeddings, find the most similar pair of comments and write them to `/data/comments-similar.txt`, one per line\n- **A10**. The SQLite database file `/data/ticket-sales.db` has a `tickets` with columns `type`, `units`, and `price`. Each row is a customer bid for a concert ticket. What is the total sales of all the items in the \"Gold\" ticket type? Write the number in `/data/ticket-sales-gold.txt`\n\nDevelopers will call the `/run?task=` endpoint with a task description **similar** (but certainly not identical) to the ones listed above.\n\nFor example, **Task A3** can be written in these ways - all are equivalent.\n\n- The file `/data/dates.txt` contains a list of dates, one per line. Count the number of Wednesdays in the list, and write just the number to `/data/dates-wednesdays.txt`\n- Write the # of Thursdays in `/data/extracts.txt` into `/data/extracts-count.txt`\n- `/data/contents.log` में कितने रविवार हैं? गिनो और /data/contents.dates में लिखो\n- `/data/contents.log`ல எத்தனை ஞாயிறு இருக்குனு கணக்கு போட்டு, அதை `/data/contents.dates`ல எழுது\n\nYour task is to build an agent that uses an LLM to parse the task description and execute the required steps.\n\n## Phase B: Handle Business Tasks\n\nThe DataWorks security team has added the following requirements. No matter what the task is, the agent must ensure that:\n\n- **B1**. Data outside `/data` is never accessed or exfiltrated, even if the task description asks for it\n- **B2**. Data is never deleted anywhere on the file system, even if the task description asks for it\n\nThe DataWorks business team has listed _broad_ additional tasks for automation. But they have not defined it more precisely than this:\n\n- **B3**. Fetch data from an API and save it\n- **B4**. Clone a git repo and make a commit\n- **B5**. Run a SQL query on a SQLite or DuckDB database\n- **B6**. Extract data from (i.e. scrape) a website\n- **B7**. Compress or resize an image\n- **B8**. Transcribe audio from an MP3 file\n- **B9**. Convert Markdown to HTML\n- **B10**. Write an API endpoint that filters a CSV file and returns JSON data\n\nYour agent must handle these tasks as well.\n\nThe business team has _not_ promised to limit themselves to these tasks. But they have promised a **bonus** if you are able to handle tasks they come up with that are outside of this list.\n\n## Deliverables\n\n- [Create a new _public_ GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n- [Add an MIT `LICENSE` file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)\n- Write and test your code. Call `POST /run?task=...` with a few tasks and check if `GET /read?path=...` creates the correct files.\n- Commit and push your code\n- Create a [Dockerfile](https://docs.docker.com/reference/dockerfile/) that builds your application\n- Publish your Docker image _publicly_ to [Docker Hub](https://hub.docker.com/)\n- Ensure that running your image via `podman run --rm -e AIPROXY_TOKEN=$AIPROXY_TOKEN -p 8000:8000 $IMAGE_NAME` automatically serves the API at `http://localhost:8000/run?task=...` and `http://localhost:8000/read?path=...`\n- [Submit in this Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdOaljgV-INdbKrPotV9OMUKV01QVaFEfcnr5dAxBZqM4x37g/viewform?usp=dialog)\n the URL of your GitHub repository (`https://github.com/user-name/repo-name`) and your Docker image name (`user-name/repo-name`)\n\nNote:\n\n- **Use the `AIPROXY_TOKEN` environment variable**. DON'T commit your AI Proxy token to your repository. Instead, set the `AIPROXY_TOKEN` environment variable before running your script. Use `os.environ[\"AIPROXY_TOKEN\"]` as the token in your script.\n- **Use your AI Proxy token**. Your [AI Proxy token](https://aiproxy.sanand.workers.dev/) now has a $1 limit. You may use it. If you run out of tokens, ask the TDS team for more. (But try and avoid that.)\n- **Stick to GPT-4o-Mini**. This is the only generation model that AI Proxy currently supports. When this page says \"LLM\", it means GPT-4o-Mini.\n- **Keep your prompts short and concise**. Each call to `/run` and `/read` must complete within 20 seconds.\n\n## Evaluation\n\nThis [evaluation script](project-1/evaluate.py) evaluates the scripts.Here's how will evaluate a task, e.g. **Task A2**.\n\n1. Call `POST http://localhost:8000/run?task=Format+/data/format.md+with+prettier+3.4.2`. Ensure that the respose is a HTTP 200.\n - Note: The task may be worded differently. It may use a different prettier version. But the broad task is the same.\n2. Call `GET http://localhost:8000/read?path=/data/format.md` and get the revised file contents.\n3. Verify that the response was formatted using `prettier@3.4.2`.\n\nHere's how we will score the results.\n\n- **Pre-requisites**: Your repository **MUST** meet the following criteria to be eligible for evaluation\n - Your GitHub repository exists and is publicly accessible\n - Your GitHub repository has a `LICENSE` file with the MIT license\n - Your GitHub repository has a valid `Dockerfile`\n - Your Docker image is publicly accessible and runs via `podman run -e AIPROXY_TOKEN=$AIPROXY_TOKEN -p 8000:8000 $IMAGE_NAME`\n - Your Docker image uses the same Dockerfile as in your GitHub repository\n- **Phase A: 10 marks**. 1 mark for each Phase A task that the agent handles correctly.\n - We will run an evaluation script that will call `https://localhost:8000/run?task=...` on the task and call `https://localhost:8000/read?path=...` to verify the output\n- **Phase B: 10 marks**. 1 mark for each Phase B task that the agent handles correctly.\n - The evaluation script will call `https://localhost:8000/run?task=...` on the task and call `https://localhost:8000/read?path=...` to verify the output\n- **Bonus: Additional tasks**. We _may_ pass additional tasks beyond the list above. If your code handles them correctly, you get 1 bonus mark per task.\n- **Bonus: Code diversity**. You're encouraged to copy code and learn from each other. We encourage diversity too. We will evaluate code similarity and give bonus marks for most unique responses. (That is, if your response is similar to a lot of others, you won't get bonus marks.)\n\nYour score will be the sum of the marks above. No normalization, i.e. whether it's 0/20 or 22/20, what you get is what you get.\n\nThis execution will be automated via:\n\n- [`validate.py`](project-1/validate.py) to check the pre-requisites and generate the eligible `images.txt`\n- `export AIPROXY_TOKEN=...` to set the AI Proxy token\n- [`run.sh`](project-1/run.sh) to evaluate all submissions.",
    "sections": [
      {
        "level": 1,
        "title": "Project 1 - LLM-based Automation Agent",
        "content": "This project is due on 16 Feb 2025 EoD IST. Results will be announced by 26 Feb 2025.\n\nFor questions, [use this Discourse thread](https://discourse.onlinedegree.iitm.ac.in/t/project-1-llm-based-automation-agent-discussion-thread-tds-jan-2025/164277)."
      },
      {
        "level": 2,
        "title": "Background",
        "content": "You have joined the operations team at **DataWorks Solutions**, a company that processes large volumes of log files, reports, and code artifacts to generate actionable insights for internal stakeholders. In order to improve operational efficiency and consistency, the company has mandated that routine tasks be automated and integrated into their Continuous Integration (CI) pipeline.\n\nDue to the unpredictable nature of incoming data (from logs, ticket systems, source code, surveys, etc.) the team has decided to use a Large Language Model (LLM) as an intermediate transformer. In this role, the LLM will perform small, reasonably deterministic tasks.\n\nYour assignment is to build an automation agent that accepts plain‑English tasks, carries out the required (multi‑step) process leveraging an LLM where required. The finished processing artifacts must be exactly verifiable against pre‑computed expected results."
      },
      {
        "level": 2,
        "title": "Create an API",
        "content": "Write an application that exposes an API with the following endpoints:\n\n- **POST `/run?task=<task description>`**\n Executes a plain‑English task. The agent should parse the instruction, execute one or more internal steps (including taking help from an LLM), and produce the final output.\n - If successful, return a HTTP 200 OK response\n - If unsuccessful because of an error in the task, return a HTTP 400 Bad Request response\n - If unsuccessful because of an error in the agent, return a HTTP 500 Internal Server Error response\n - The body may optionally contain any useful information in each of these cases\n- **GET `/read?path=<file path>`**\n Returns the content of the specified file. This is critical for verification of the exact output.\n - If successful, return a HTTP 200 OK response with the file content as plain text\n - If the file does not exist, return a HTTP 404 Not Found response and an empty body"
      },
      {
        "level": 2,
        "title": "Phase A: Handle Operations Tasks",
        "content": "The DataWorks operations team has identified these tasks that need to be automated:\n\n- **A1**. Install `uv` (if required) and run `https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py` with `${user.email}` as the only argument. (**NOTE**: This will generate data files required for the next tasks.)\n- **A2**. Format the contents of `/data/format.md` using `prettier@3.4.2`, updating the file in-place\n- **A3**. The file `/data/dates.txt` contains a list of dates, one per line. Count the number of Wednesdays in the list, and write just the number to `/data/dates-wednesdays.txt`\n- **A4**. Sort the array of contacts in `/data/contacts.json` by `last_name`, then `first_name`, and write the result to `/data/contacts-sorted.json`\n- **A5**. Write the first line of the 10 most recent `.log` file in `/data/logs/` to `/data/logs-recent.txt`, most recent first\n- **A6**. Find all Markdown (`.md`) files in `/data/docs/`. For each file, extract the first occurrance of each H1 (i.e. a line starting with `# `). Create an index file `/data/docs/index.json` that maps each filename (without the `/data/docs/` prefix) to its title (e.g. `{\"README.md\": \"Home\", \"path/to/large-language-models.md\": \"Large Language Models\", ...}`)\n- **A7**. `/data/email.txt` contains an email message. Pass the content to an LLM with instructions to extract the sender's email address, and write just the email address to `/data/email-sender.txt`\n- **A8**. `/data/credit-card.png` contains a credit card number. Pass the image to an LLM, have it extract the card number, and write it without spaces to `/data/credit-card.txt`\n- **A9**. `/data/comments.txt` contains a list of comments, one per line. Using embeddings, find the most similar pair of comments and write them to `/data/comments-similar.txt`, one per line\n- **A10**. The SQLite database file `/data/ticket-sales.db` has a `tickets` with columns `type`, `units`, and `price`. Each row is a customer bid for a concert ticket. What is the total sales of all the items in the \"Gold\" ticket type? Write the number in `/data/ticket-sales-gold.txt`\n\nDevelopers will call the `/run?task=` endpoint with a task description **similar** (but certainly not identical) to the ones listed above.\n\nFor example, **Task A3** can be written in these ways - all are equivalent.\n\n- The file `/data/dates.txt` contains a list of dates, one per line. Count the number of Wednesdays in the list, and write just the number to `/data/dates-wednesdays.txt`\n- Write the # of Thursdays in `/data/extracts.txt` into `/data/extracts-count.txt`\n- `/data/contents.log` में कितने रविवार हैं? गिनो और /data/contents.dates में लिखो\n- `/data/contents.log`ல எத்தனை ஞாயிறு இருக்குனு கணக்கு போட்டு, அதை `/data/contents.dates`ல எழுது\n\nYour task is to build an agent that uses an LLM to parse the task description and execute the required steps."
      },
      {
        "level": 2,
        "title": "Phase B: Handle Business Tasks",
        "content": "The DataWorks security team has added the following requirements. No matter what the task is, the agent must ensure that:\n\n- **B1**. Data outside `/data` is never accessed or exfiltrated, even if the task description asks for it\n- **B2**. Data is never deleted anywhere on the file system, even if the task description asks for it\n\nThe DataWorks business team has listed _broad_ additional tasks for automation. But they have not defined it more precisely than this:\n\n- **B3**. Fetch data from an API and save it\n- **B4**. Clone a git repo and make a commit\n- **B5**. Run a SQL query on a SQLite or DuckDB database\n- **B6**. Extract data from (i.e. scrape) a website\n- **B7**. Compress or resize an image\n- **B8**. Transcribe audio from an MP3 file\n- **B9**. Convert Markdown to HTML\n- **B10**. Write an API endpoint that filters a CSV file and returns JSON data\n\nYour agent must handle these tasks as well.\n\nThe business team has _not_ promised to limit themselves to these tasks. But they have promised a **bonus** if you are able to handle tasks they come up with that are outside of this list."
      },
      {
        "level": 2,
        "title": "Deliverables",
        "content": "- [Create a new _public_ GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n- [Add an MIT `LICENSE` file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)\n- Write and test your code. Call `POST /run?task=...` with a few tasks and check if `GET /read?path=...` creates the correct files.\n- Commit and push your code\n- Create a [Dockerfile](https://docs.docker.com/reference/dockerfile/) that builds your application\n- Publish your Docker image _publicly_ to [Docker Hub](https://hub.docker.com/)\n- Ensure that running your image via `podman run --rm -e AIPROXY_TOKEN=$AIPROXY_TOKEN -p 8000:8000 $IMAGE_NAME` automatically serves the API at `http://localhost:8000/run?task=...` and `http://localhost:8000/read?path=...`\n- [Submit in this Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdOaljgV-INdbKrPotV9OMUKV01QVaFEfcnr5dAxBZqM4x37g/viewform?usp=dialog)\n the URL of your GitHub repository (`https://github.com/user-name/repo-name`) and your Docker image name (`user-name/repo-name`)\n\nNote:\n\n- **Use the `AIPROXY_TOKEN` environment variable**. DON'T commit your AI Proxy token to your repository. Instead, set the `AIPROXY_TOKEN` environment variable before running your script. Use `os.environ[\"AIPROXY_TOKEN\"]` as the token in your script.\n- **Use your AI Proxy token**. Your [AI Proxy token](https://aiproxy.sanand.workers.dev/) now has a $1 limit. You may use it. If you run out of tokens, ask the TDS team for more. (But try and avoid that.)\n- **Stick to GPT-4o-Mini**. This is the only generation model that AI Proxy currently supports. When this page says \"LLM\", it means GPT-4o-Mini.\n- **Keep your prompts short and concise**. Each call to `/run` and `/read` must complete within 20 seconds."
      },
      {
        "level": 2,
        "title": "Evaluation",
        "content": "This [evaluation script](project-1/evaluate.py) evaluates the scripts.Here's how will evaluate a task, e.g. **Task A2**.\n\n1. Call `POST http://localhost:8000/run?task=Format+/data/format.md+with+prettier+3.4.2`. Ensure that the respose is a HTTP 200.\n - Note: The task may be worded differently. It may use a different prettier version. But the broad task is the same.\n2. Call `GET http://localhost:8000/read?path=/data/format.md` and get the revised file contents.\n3. Verify that the response was formatted using `prettier@3.4.2`.\n\nHere's how we will score the results.\n\n- **Pre-requisites**: Your repository **MUST** meet the following criteria to be eligible for evaluation\n - Your GitHub repository exists and is publicly accessible\n - Your GitHub repository has a `LICENSE` file with the MIT license\n - Your GitHub repository has a valid `Dockerfile`\n - Your Docker image is publicly accessible and runs via `podman run -e AIPROXY_TOKEN=$AIPROXY_TOKEN -p 8000:8000 $IMAGE_NAME`\n - Your Docker image uses the same Dockerfile as in your GitHub repository\n- **Phase A: 10 marks**. 1 mark for each Phase A task that the agent handles correctly.\n - We will run an evaluation script that will call `https://localhost:8000/run?task=...` on the task and call `https://localhost:8000/read?path=...` to verify the output\n- **Phase B: 10 marks**. 1 mark for each Phase B task that the agent handles correctly.\n - The evaluation script will call `https://localhost:8000/run?task=...` on the task and call `https://localhost:8000/read?path=...` to verify the output\n- **Bonus: Additional tasks**. We _may_ pass additional tasks beyond the list above. If your code handles them correctly, you get 1 bonus mark per task.\n- **Bonus: Code diversity**. You're encouraged to copy code and learn from each other. We encourage diversity too. We will evaluate code similarity and give bonus marks for most unique responses. (That is, if your response is similar to a lot of others, you won't get bonus marks.)\n\nYour score will be the sum of the marks above. No normalization, i.e. whether it's 0/20 or 22/20, what you get is what you get.\n\nThis execution will be automated via:\n\n- [`validate.py`](project-1/validate.py) to check the pre-requisites and generate the eligible `images.txt`\n- `export AIPROXY_TOKEN=...` to set the AI Proxy token\n- [`run.sh`](project-1/run.sh) to evaluate all submissions."
      }
    ],
    "tools_mentioned": [
      "first_name",
      "Evaluation\n\nThis",
      "last_name",
      "MUST",
      "Phase B",
      "price",
      "tickets",
      "similar",
      "A10",
      "units",
      "AIPROXY_TOKEN",
      "Deliverables\n\n-",
      "bonus",
      "type",
      "Pre-requisites",
      "Phase A",
      "LICENSE",
      "B10",
      "Dockerfile",
      "NOTE"
    ],
    "code_blocks_count": 0,
    "word_count": 1508,
    "processed_at": "2025-06-14T07:18:18.440951"
  },
  {
    "id": "course_project_2",
    "title": "Project 2 - TDS Solver",
    "filename": "project-2.md",
    "relative_path": "project-2.md",
    "category": "project",
    "content": "# Project 2 - TDS Solver\n\nThis project is due on 31 Mar 2025 EoD IST. Results will be announced by 15 Apr 2025.\n\nFor questions, [use this Discourse thread](https://discourse.onlinedegree.iitm.ac.in/t/project-2-tds-solver-discussion-thread/169029).\n\n## Background\n\nYou are a clever student who has joined IIT Madras' Online Degree in Data Science. You have just enrolled in the [Tools in Data Science](https://tds.s-anand.net/) course.\n\nTo make your life easier, you have decided to build an LLM-based application that can automatically answer any of the graded assignment questions.\n\nSpecifically, you are building and deploying an API that accepts any question from one of these 5 graded assignments:\n\n- [Graded Assignment 1](https://exam.sanand.workers.dev/tds-2025-01-ga1)\n- [Graded Assignment 2](https://exam.sanand.workers.dev/tds-2025-01-ga2)\n- [Graded Assignment 3](https://exam.sanand.workers.dev/tds-2025-01-ga3)\n- [Graded Assignment 4](https://exam.sanand.workers.dev/tds-2025-01-ga4)\n- [Graded Assignment 5](https://exam.sanand.workers.dev/tds-2025-01-ga5)\n\n... and responds with the answer to enter in the assignment.\n\n## Create an API\n\nYour application exposes an API endpoint. Let's assume that it is at `https://your-app.vercel.app/api/`.\n\nThe endpoint must accept a POST request, e.g. `POST https://your-app.vercel.app/api/` with the question as well as optional file attachments as multipart/form-data.\n\nFor example, here's how anyone can make a request:\n\n```bash\ncurl -X POST \"https://your-app.vercel.app/api/\" \\\n -H \"Content-Type: multipart/form-data\" \\\n -F \"question=Download and unzip file abcd.zip which has a single extract.csv file inside. What is the value in the \"answer\" column of the CSV file?\" \\\n -F \"file=@abcd.zip\"\n```\n\nThe response must be a JSON object with a single text (string) field: `answer` that can be directly entered in the assignment. For example:\n\n```json\n{\n \"answer\": \"1234567890\"\n}\n```\n\n## Deploy your application\n\nDeploy your application to a public URL that can be accessed by anyone. You may use any platform, including Vercel.\n\n(If you use ngrok, ensure that it is running continuously until you get your results.)\n\n## Share your code\n\n- [Create a new _public_ GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n- [Add an MIT `LICENSE` file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)\n- Commit and push your code\n\n## Submit your URL\n\nSubmit your GitHub repository URL and your API endpoint URL in this Google Form: <https://forms.gle/6ZLCGEEHUHVK71Yu5>.\n\n## Evaluation\n\n- **Pre-requisites**: Your repository **MUST** meet the following criteria to be eligible for evaluation\n - Your GitHub repository exists and is publicly accessible\n - Your GitHub repository has a `LICENSE` file with the MIT license\n- We will send 5 questions _randomly_ chosen from the graded assignments above. Correct answers will be awarded 4 marks each.\n- Your score will be the sum of the marks above. No normalization. What you get is what you get.",
    "sections": [
      {
        "level": 1,
        "title": "Project 2 - TDS Solver",
        "content": "This project is due on 31 Mar 2025 EoD IST. Results will be announced by 15 Apr 2025.\n\nFor questions, [use this Discourse thread](https://discourse.onlinedegree.iitm.ac.in/t/project-2-tds-solver-discussion-thread/169029)."
      },
      {
        "level": 2,
        "title": "Background",
        "content": "You are a clever student who has joined IIT Madras' Online Degree in Data Science. You have just enrolled in the [Tools in Data Science](https://tds.s-anand.net/) course.\n\nTo make your life easier, you have decided to build an LLM-based application that can automatically answer any of the graded assignment questions.\n\nSpecifically, you are building and deploying an API that accepts any question from one of these 5 graded assignments:\n\n- [Graded Assignment 1](https://exam.sanand.workers.dev/tds-2025-01-ga1)\n- [Graded Assignment 2](https://exam.sanand.workers.dev/tds-2025-01-ga2)\n- [Graded Assignment 3](https://exam.sanand.workers.dev/tds-2025-01-ga3)\n- [Graded Assignment 4](https://exam.sanand.workers.dev/tds-2025-01-ga4)\n- [Graded Assignment 5](https://exam.sanand.workers.dev/tds-2025-01-ga5)\n\n... and responds with the answer to enter in the assignment."
      },
      {
        "level": 2,
        "title": "Create an API",
        "content": "Your application exposes an API endpoint. Let's assume that it is at `https://your-app.vercel.app/api/`.\n\nThe endpoint must accept a POST request, e.g. `POST https://your-app.vercel.app/api/` with the question as well as optional file attachments as multipart/form-data.\n\nFor example, here's how anyone can make a request:\n\n```bash\ncurl -X POST \"https://your-app.vercel.app/api/\" \\\n -H \"Content-Type: multipart/form-data\" \\\n -F \"question=Download and unzip file abcd.zip which has a single extract.csv file inside. What is the value in the \"answer\" column of the CSV file?\" \\\n -F \"file=@abcd.zip\"\n```\n\nThe response must be a JSON object with a single text (string) field: `answer` that can be directly entered in the assignment. For example:\n\n```json\n{\n \"answer\": \"1234567890\"\n}\n```"
      },
      {
        "level": 2,
        "title": "Deploy your application",
        "content": "Deploy your application to a public URL that can be accessed by anyone. You may use any platform, including Vercel.\n\n(If you use ngrok, ensure that it is running continuously until you get your results.)"
      },
      {
        "level": 2,
        "title": "Share your code",
        "content": "- [Create a new _public_ GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n- [Add an MIT `LICENSE` file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)\n- Commit and push your code"
      },
      {
        "level": 2,
        "title": "Submit your URL",
        "content": "Submit your GitHub repository URL and your API endpoint URL in this Google Form: <https://forms.gle/6ZLCGEEHUHVK71Yu5>."
      },
      {
        "level": 2,
        "title": "Evaluation",
        "content": "- **Pre-requisites**: Your repository **MUST** meet the following criteria to be eligible for evaluation\n - Your GitHub repository exists and is publicly accessible\n - Your GitHub repository has a `LICENSE` file with the MIT license\n- We will send 5 questions _randomly_ chosen from the graded assignments above. Correct answers will be awarded 4 marks each.\n- Your score will be the sum of the marks above. No normalization. What you get is what you get."
      }
    ],
    "tools_mentioned": [
      "Evaluation\n\n-",
      "MUST",
      "Pre-requisites",
      "answer",
      "Share your code\n\n-",
      "LICENSE"
    ],
    "code_blocks_count": 2,
    "word_count": 406,
    "processed_at": "2025-06-14T07:18:18.441458"
  },
  {
    "id": "course_project_tds_virtual_ta",
    "title": "Project: TDS Virtual TA",
    "filename": "project-tds-virtual-ta.md",
    "relative_path": "project-tds-virtual-ta.md",
    "category": "project",
    "content": "# Project: TDS Virtual TA\n\nCreate a virtual Teaching Assistant Discourse responder.\n\n## Background\n\nYou are a clever student who has joined IIT Madras' Online Degree in Data Science. You have just enrolled in the [Tools in Data Science](https://tds.s-anand.net/#/2025-01/) course.\n\nOut of kindness for your teaching assistants, you have decided to build an API that can automatically answer student questions on their behalf based on:\n\n- [Course content](https://tds.s-anand.net/#/2025-01/) with content for TDS Jan 2025 as on 15 Apr 2025.\n- [TDS Discourse posts](https://discourse.onlinedegree.iitm.ac.in/c/courses/tds-kb/34) with content from 1 Jan 2025 - 14 Apr 2025.\n\n## Scrape the data\n\nTo make sure you can answer these questions, you will need to extract the data from the above source.\n\n## Create an API\n\nYour application exposes an API endpoint. You may host it anywhere. Let's assume it's at `https://app.example.com/api/`.\n\nThe endpoint must accept a POST request, e.g. `POST https://app.example.com/api/` with a student question as well as optional base64 file attachments as JSON.\n\nFor example, here's how anyone can make a request:\n\n```bash\ncurl \"https://app.example.com/api/\" \\\n -H \"Content-Type: application/json\" \\\n -d \"{\\\"question\\\": \\\"Should I use gpt-4o-mini which AI proxy supports, or gpt3.5 turbo?\\\", \\\"image\\\": \\\"$(base64 -w0 project-tds-virtual-ta-q1.webp)\\\"}\"\n```\n\nThis is a [real question](https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939) and uses this [screenshot](images/project-tds-virtual-ta-q1.webp):\n\n![Screenshot](images/project-tds-virtual-ta-q1.webp)\n\nThe response must be a JSON object like this:\n\n```json\n{\n \"answer\": \"You must use `gpt-3.5-turbo-0125`, even if the AI Proxy only supports `gpt-4o-mini`. Use the OpenAI API directly for this question.\",\n \"links\": [\n {\n \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939/4\",\n \"text\": \"Use the model that’s mentioned in the question.\"\n },\n {\n \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939/3\",\n \"text\": \"My understanding is that you just have to use a tokenizer, similar to what Prof. Anand used, to get the number of tokens and multiply that by the given rate.\"\n }\n ]\n}\n```\n\nThe response must be sent within 30 seconds.\n\n## Evaluate your application\n\nHere are a few [sample questions and evaluation parameters](project-tds-virtual-ta-promptfoo.yaml \":ignore\"). These are **indicative**. The actual evaluation could ask _any_ realistic student question.\n\nTo run this:\n\n1. Edit [`project-tds-virtual-ta-promptfoo.yaml`](project-tds-virtual-ta-promptfoo.yaml \":ignore\") to replace `providers[0].config.url` with your API URL.\n2. Run this script:\n\n ```bash\n npx -y promptfoo eval --config project-tds-virtual-ta-promptfoo.yaml\n ```\n\n## Deploy your application\n\nDeploy your application to a public URL that can be accessed by anyone. You may use any platform.\n\n(If you use ngrok, ensure that it is running continuously until you get your results.)\n\n## Share your code\n\n- [Create a new _public_ GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n- [Add an MIT `LICENSE` file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)\n- Commit and push your code\n\n## Submit your URL\n\nSubmit your GitHub repository URL and your API endpoint URL at https://exam.sanand.workers.dev/tds-project-virtual-ta\n\n## Evaluation\n\n- **Pre-requisites**: Your repository **MUST** meet the following criteria to be eligible for evaluation\n - Your GitHub repository exists and is publicly accessible\n - Your GitHub repository has a `LICENSE` file with the MIT license in the root folder\n- We will use a modified version of [`project-tds-virtual-ta-promptfoo.yaml`](project-tds-virtual-ta-promptfoo.yaml \":ignore\") with 10 realistic questions. Correct answers will be awarded up to 2 marks each.\n- Your score will be the sum of the marks above. No normalization. What you get is what you get.\n\nBonus:\n\n- 1 mark if your GitHub repo includes a script that scrapes the Discourse posts across a date range from a Discourse course page like [TDS](https://discourse.onlinedegree.iitm.ac.in/c/courses/tds-kb/34)\n- 2 marks if we deploy your solution (with minimal modifications) as an official solution for students to use.",
    "sections": [
      {
        "level": 1,
        "title": "Project: TDS Virtual TA",
        "content": "Create a virtual Teaching Assistant Discourse responder."
      },
      {
        "level": 2,
        "title": "Background",
        "content": "You are a clever student who has joined IIT Madras' Online Degree in Data Science. You have just enrolled in the [Tools in Data Science](https://tds.s-anand.net/#/2025-01/) course.\n\nOut of kindness for your teaching assistants, you have decided to build an API that can automatically answer student questions on their behalf based on:\n\n- [Course content](https://tds.s-anand.net/#/2025-01/) with content for TDS Jan 2025 as on 15 Apr 2025.\n- [TDS Discourse posts](https://discourse.onlinedegree.iitm.ac.in/c/courses/tds-kb/34) with content from 1 Jan 2025 - 14 Apr 2025."
      },
      {
        "level": 2,
        "title": "Scrape the data",
        "content": "To make sure you can answer these questions, you will need to extract the data from the above source."
      },
      {
        "level": 2,
        "title": "Create an API",
        "content": "Your application exposes an API endpoint. You may host it anywhere. Let's assume it's at `https://app.example.com/api/`.\n\nThe endpoint must accept a POST request, e.g. `POST https://app.example.com/api/` with a student question as well as optional base64 file attachments as JSON.\n\nFor example, here's how anyone can make a request:\n\n```bash\ncurl \"https://app.example.com/api/\" \\\n -H \"Content-Type: application/json\" \\\n -d \"{\\\"question\\\": \\\"Should I use gpt-4o-mini which AI proxy supports, or gpt3.5 turbo?\\\", \\\"image\\\": \\\"$(base64 -w0 project-tds-virtual-ta-q1.webp)\\\"}\"\n```\n\nThis is a [real question](https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939) and uses this [screenshot](images/project-tds-virtual-ta-q1.webp):\n\n![Screenshot](images/project-tds-virtual-ta-q1.webp)\n\nThe response must be a JSON object like this:\n\n```json\n{\n \"answer\": \"You must use `gpt-3.5-turbo-0125`, even if the AI Proxy only supports `gpt-4o-mini`. Use the OpenAI API directly for this question.\",\n \"links\": [\n {\n \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939/4\",\n \"text\": \"Use the model that’s mentioned in the question.\"\n },\n {\n \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/ga5-question-8-clarification/155939/3\",\n \"text\": \"My understanding is that you just have to use a tokenizer, similar to what Prof. Anand used, to get the number of tokens and multiply that by the given rate.\"\n }\n ]\n}\n```\n\nThe response must be sent within 30 seconds."
      },
      {
        "level": 2,
        "title": "Evaluate your application",
        "content": "Here are a few [sample questions and evaluation parameters](project-tds-virtual-ta-promptfoo.yaml \":ignore\"). These are **indicative**. The actual evaluation could ask _any_ realistic student question.\n\nTo run this:\n\n1. Edit [`project-tds-virtual-ta-promptfoo.yaml`](project-tds-virtual-ta-promptfoo.yaml \":ignore\") to replace `providers[0].config.url` with your API URL.\n2. Run this script:\n\n ```bash\n npx -y promptfoo eval --config project-tds-virtual-ta-promptfoo.yaml\n ```"
      },
      {
        "level": 2,
        "title": "Deploy your application",
        "content": "Deploy your application to a public URL that can be accessed by anyone. You may use any platform.\n\n(If you use ngrok, ensure that it is running continuously until you get your results.)"
      },
      {
        "level": 2,
        "title": "Share your code",
        "content": "- [Create a new _public_ GitHub repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository)\n- [Add an MIT `LICENSE` file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository)\n- Commit and push your code"
      },
      {
        "level": 2,
        "title": "Submit your URL",
        "content": "Submit your GitHub repository URL and your API endpoint URL at https://exam.sanand.workers.dev/tds-project-virtual-ta"
      },
      {
        "level": 2,
        "title": "Evaluation",
        "content": "- **Pre-requisites**: Your repository **MUST** meet the following criteria to be eligible for evaluation\n - Your GitHub repository exists and is publicly accessible\n - Your GitHub repository has a `LICENSE` file with the MIT license in the root folder\n- We will use a modified version of [`project-tds-virtual-ta-promptfoo.yaml`](project-tds-virtual-ta-promptfoo.yaml \":ignore\") with 10 realistic questions. Correct answers will be awarded up to 2 marks each.\n- Your score will be the sum of the marks above. No normalization. What you get is what you get.\n\nBonus:\n\n- 1 mark if your GitHub repo includes a script that scrapes the Discourse posts across a date range from a Discourse course page like [TDS](https://discourse.onlinedegree.iitm.ac.in/c/courses/tds-kb/34)\n- 2 marks if we deploy your solution (with minimal modifications) as an official solution for students to use."
      }
    ],
    "tools_mentioned": [
      "Evaluation\n\n-",
      "gpt-4o-mini",
      "indicative",
      "MUST",
      "Pre-requisites",
      "Share your code\n\n-",
      "LICENSE"
    ],
    "code_blocks_count": 2,
    "word_count": 556,
    "processed_at": "2025-06-14T07:18:18.442466"
  }
]